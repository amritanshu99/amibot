{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031d09b3-9d4c-447b-9889-64b56f81e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All files saved to 'amibot_data/' for light Flask usage.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ Install required packages (if needed)\n",
    "# !pip install -q sentence-transformers nltk pandas torch rapidfuzz\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ðŸ“Œ Download WordNet for synonym expansion\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "csv_path = \"amibot.csv\"  # Ensure it has columns: 'Field', 'Value'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_path, encoding='cp1252')  # fallback encoding\n",
    "\n",
    "# âœ… Text cleaning\n",
    "def correct_typos(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# âœ… Synonym expansion using WordNet\n",
    "def expand_with_synonyms(text):\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    for word in words:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
    "        if synonyms:\n",
    "            expanded_words.append(word + \" \" + \" \".join(list(synonyms)[:2]))\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "    return \" \".join(expanded_words)\n",
    "\n",
    "# âœ… Build field variant map\n",
    "field_map = {}\n",
    "field_variants = {}  # key = canonical field, value = list of variants\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    field_str = row[\"Field\"]\n",
    "    response = row[\"Value\"]\n",
    "\n",
    "    variants = [v.strip().lower() for v in field_str.split(\",\") if v.strip()]\n",
    "    for variant in variants:\n",
    "        field_map[variant] = response\n",
    "\n",
    "    canonical = variants[0]\n",
    "    field_variants[canonical] = variants\n",
    "\n",
    "# âœ… Generate embeddings for all variants (with expansion)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "query_list = []\n",
    "variant_to_response = {}\n",
    "\n",
    "for canonical, variants in field_variants.items():\n",
    "    for v in variants:\n",
    "        cleaned = correct_typos(v)\n",
    "        expanded = expand_with_synonyms(cleaned)\n",
    "        query_list.append(expanded)\n",
    "        variant_to_response[expanded] = field_map[v]  # maps expanded input to response\n",
    "\n",
    "# âœ… Encode all queries\n",
    "embeddings = model.encode(query_list, convert_to_tensor=True)\n",
    "\n",
    "# âœ… Save everything\n",
    "os.makedirs(\"amibot_data\", exist_ok=True)\n",
    "\n",
    "torch.save(embeddings, \"amibot_data/field_embeddings.pt\")\n",
    "with open(\"amibot_data/query_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(query_list, f)\n",
    "\n",
    "with open(\"amibot_data/variant_to_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variant_to_response, f)\n",
    "\n",
    "print(\"âœ… All files saved to 'amibot_data/' for light Flask usage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b37998-b93d-4a4b-b937-e4a5f18ee050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
