{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4703d-9222-4575-bfac-9347f95ae6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amrit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading transformer model...\n",
      "âœ… Loaded CSV with encoding: Index(['Field', 'Value'], dtype='object')\n",
      "ğŸ’¾ Saved df.pkl, field_variants.pkl, and field_embeddings.pt to 'amibot_data/'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask AmiBot (type 'exit' to quit):  wife\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Matched: 'wife'\n",
      "ğŸ“ Semantic: 0.75, ğŸ”¤ Fuzzy: 100.0\n",
      "ğŸ‘‰ Committed for life â€” merged with Sneha Mishra in a lifelong partnership.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Install required packages (run once)\n",
    "!pip install -q sentence-transformers rapidfuzz nltk\n",
    "\n",
    "# ğŸ“¥ Imports\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz import fuzz\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ğŸ“Œ Download NLTK corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# ğŸ§  Load transformer model\n",
    "print(\"ğŸ“¥ Loading transformer model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ğŸ“„ Load your CSV file (replace path if needed)\n",
    "csv_path = \"amibot.csv\"  # Ensure it has columns: 'Field', 'Value'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_path, encoding='cp1252')  # fallback encoding\n",
    "\n",
    "print(\"âœ… Loaded CSV with encoding:\", df.columns)\n",
    "\n",
    "# ğŸ“š Preprocess data\n",
    "field_variants = []\n",
    "field_map = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    field_str = row[\"Field\"]\n",
    "    value = row[\"Value\"]\n",
    "    variants = [v.strip().lower() for v in field_str.split(\",\") if v.strip()]\n",
    "    for v in variants:\n",
    "        field_variants.append(v)\n",
    "        field_map[v] = value  # Map each variant to its value\n",
    "\n",
    "field_embeddings = model.encode(field_variants, convert_to_tensor=True)\n",
    "\n",
    "# ğŸ”§ Function: Correct typos (basic spell fix using regex for now)\n",
    "def correct_typos(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# ğŸ”§ Function: Expand with synonyms using WordNet\n",
    "def expand_with_synonyms(text):\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    for word in words:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
    "        if synonyms:\n",
    "            expanded_words.append(word + \" \" + \" \".join(list(synonyms)[:2]))\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "    return \" \".join(expanded_words)\n",
    "\n",
    "# ğŸ¤– Function: Get AmiBot response\n",
    "def get_response(user_input, model, field_variants, field_embeddings, field_map, threshold=0.55, fuzz_threshold=55):\n",
    "    original_input = user_input.strip()\n",
    "    corrected_input = correct_typos(original_input)\n",
    "    expanded_input = expand_with_synonyms(corrected_input)\n",
    "\n",
    "    query_embedding = model.encode(expanded_input, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(query_embedding, field_embeddings)[0]\n",
    "\n",
    "    best_score = float(similarities.max())\n",
    "    best_idx = int(similarities.argmax())\n",
    "    best_field = field_variants[best_idx]\n",
    "    best_answer = field_map[best_field]\n",
    "\n",
    "    fuzzy_score = fuzz.token_set_ratio(original_input.lower(), best_field.lower())\n",
    "\n",
    "    if best_score >= threshold or fuzzy_score >= fuzz_threshold:\n",
    "        return f\"\\nâœ… Matched: '{best_field}'\\nğŸ“ Semantic: {best_score:.2f}, ğŸ”¤ Fuzzy: {fuzzy_score}\\nğŸ‘‰ {best_answer}\"\n",
    "    else:\n",
    "        return f\"\\nğŸ¤– Sorry, Iâ€™m not sure what you meant.\\nğŸ’¡ Did you mean: '{best_field}'?\\nPlease rephrase your question.\"\n",
    "\n",
    "# ğŸ’¾ Save necessary components for Flask app\n",
    "save_dir = \"amibot_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{save_dir}/df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "with open(f\"{save_dir}/field_variants.pkl\", \"wb\") as f:\n",
    "    pickle.dump(field_variants, f)\n",
    "\n",
    "with open(f\"{save_dir}/field_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(field_map, f)\n",
    "\n",
    "torch.save(field_embeddings, f\"{save_dir}/field_embeddings.pt\")\n",
    "\n",
    "print(\"ğŸ’¾ Saved df.pkl, field_variants.pkl, and field_embeddings.pt to 'amibot_data/'\")\n",
    "\n",
    "# ğŸ§ª Test in Notebook (example)\n",
    "while True:\n",
    "    user_input = input(\"\\nAsk AmiBot (type 'exit' to quit): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    response = get_response(user_input, model, field_variants, field_embeddings, field_map)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c90978-973d-412f-a4c7-9dffc42cb2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
